{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37cc58f6-baa4-48b8-8378-b764e1c94bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, Input, LeakyReLU, GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dabd686e-58b8-4474-86dc-e98a09a0eb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34080</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,362,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34080\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m4,362,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │         \u001b[38;5;34m4,257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,368,929</span> (16.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,368,929\u001b[0m (16.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,368,929</span> (16.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,368,929\u001b[0m (16.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - categorical_accuracy: 0.3586 - loss: 3.9936 - val_categorical_accuracy: 0.8836 - val_loss: 0.3923\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - categorical_accuracy: 0.9088 - loss: 0.3035 - val_categorical_accuracy: 0.9133 - val_loss: 0.2549\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - categorical_accuracy: 0.9376 - loss: 0.2027 - val_categorical_accuracy: 0.9168 - val_loss: 0.2393\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - categorical_accuracy: 0.9538 - loss: 0.1532 - val_categorical_accuracy: 0.9455 - val_loss: 0.1828\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - categorical_accuracy: 0.9667 - loss: 0.1048 - val_categorical_accuracy: 0.9420 - val_loss: 0.1700\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - categorical_accuracy: 0.9683 - loss: 0.1036 - val_categorical_accuracy: 0.9427 - val_loss: 0.1787\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - categorical_accuracy: 0.9726 - loss: 0.0932 - val_categorical_accuracy: 0.9485 - val_loss: 0.1670\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - categorical_accuracy: 0.9820 - loss: 0.0564 - val_categorical_accuracy: 0.9536 - val_loss: 0.1481\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - categorical_accuracy: 0.9890 - loss: 0.0359 - val_categorical_accuracy: 0.9478 - val_loss: 0.1611\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - categorical_accuracy: 0.9877 - loss: 0.0411 - val_categorical_accuracy: 0.9532 - val_loss: 0.1449\n",
      "Categorical Accuracy: 95.32%\n"
     ]
    }
   ],
   "source": [
    "with open('TCGA_new_pre_second.pckl', 'rb') as file_second:\n",
    "    (\n",
    "        dropped_genes_final,\n",
    "        dropped_gene_name,\n",
    "        dropped_Ens_id,\n",
    "        samp_id_new,\n",
    "        diag_name_new,\n",
    "        project_ids_new\n",
    "    ) = pd.compat.pickle_compat.load(file_second)\n",
    "\n",
    "with open('TCGA_new_pre_first.pckl', 'rb') as file_first:\n",
    "    _, _, _, _, remain_cancer_ids_ind, remain_normal_ids_ind = pickle.load(file_first)\n",
    "\n",
    "# 2. Encode Labels\n",
    "# Integer Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(project_ids_new)\n",
    "# One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded_reshaped = integer_encoded.reshape(-1, 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded_reshaped)\n",
    "# 3. Prepare Cancer Samples\n",
    "X_cancer_samples = dropped_genes_final.iloc[:, remain_cancer_ids_ind].T.values\n",
    "onehot_encoded_cancer_samples = onehot_encoded[remain_cancer_ids_ind]\n",
    "# Add Nine Zeros to Each Sample\n",
    "X_cancer_samples_mat = np.concatenate(\n",
    "    (X_cancer_samples, np.zeros((X_cancer_samples.shape[0], 9))),\n",
    "    axis=1\n",
    ")\n",
    "# Ensure the total number of features is divisible by 71 and 100\n",
    "assert X_cancer_samples_mat.shape[1] % (71 * 100) == 0, \"Reshape not possible with current dimensions.\"\n",
    "# Reshape to (num_samples, 71, 100)\n",
    "X_cancer_samples_mat = X_cancer_samples_mat.reshape(-1, 71, 100)\n",
    "\n",
    "# 4. Split Data into Training and Testing Sets\n",
    "# Use integer labels for stratification\n",
    "\n",
    "y_labels = integer_encoded[remain_cancer_ids_ind]\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X_cancer_samples_mat,\n",
    "    onehot_encoded_cancer_samples,\n",
    "    stratify=y_labels,  # Corrected stratify parameter\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Define Parameters\n",
    "img_rows, img_cols = x_test.shape[1], x_test.shape[2]\n",
    "num_classes = y_train.shape[1]\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 6. Reshape Data for CNN\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "\n",
    "# 7. Build the CNN Model\n",
    "def convolutional_model():\n",
    "    model = Sequential()\n",
    "    # First Convolutional Layer\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(1, 71),\n",
    "            strides=(1, 1),\n",
    "            input_shape=(img_rows, img_cols, 1)  # Ensure input_shape is correctly set\n",
    "        )\n",
    "    )\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "\n",
    "    # Flatten and Dense Layers for Classification\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "# 8. Compile the Model\n",
    "model = convolutional_model()\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Display Model Architecture\n",
    "model.summary()\n",
    "# 9. Set Up Early Stopping\n",
    "callbacks = [EarlyStopping(monitor='categorical_accuracy', patience=3, verbose=0)]\n",
    "# 10. Train the Model\n",
    "history = model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,callbacks=callbacks,validation_data=(x_test, y_test))\n",
    "\n",
    "# 11. Evaluate the Model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Categorical Accuracy: {scores[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6bad90-05d3-4ff9-b872-83bb56c58823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2982</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,439</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,122</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m84\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │            \u001b[38;5;34m18\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m84\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2982\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │        \u001b[38;5;34m98,439\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │         \u001b[38;5;34m1,122\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,579</span> (388.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m99,579\u001b[0m (388.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,579</span> (388.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m99,579\u001b[0m (388.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - categorical_accuracy: 0.0697 - loss: 4.5520 - val_categorical_accuracy: 0.1509 - val_loss: 2.9322\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - categorical_accuracy: 0.1858 - loss: 2.8521 - val_categorical_accuracy: 0.2213 - val_loss: 2.6705\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - categorical_accuracy: 0.2128 - loss: 2.6301 - val_categorical_accuracy: 0.2120 - val_loss: 2.5173\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - categorical_accuracy: 0.2196 - loss: 2.4793 - val_categorical_accuracy: 0.2190 - val_loss: 2.4237\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - categorical_accuracy: 0.2280 - loss: 2.4028 - val_categorical_accuracy: 0.2561 - val_loss: 2.3418\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - categorical_accuracy: 0.2686 - loss: 2.3046 - val_categorical_accuracy: 0.2696 - val_loss: 2.2724\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - categorical_accuracy: 0.3066 - loss: 2.2311 - val_categorical_accuracy: 0.3327 - val_loss: 2.2063\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - categorical_accuracy: 0.3380 - loss: 2.1694 - val_categorical_accuracy: 0.3613 - val_loss: 2.1162\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - categorical_accuracy: 0.3657 - loss: 2.0843 - val_categorical_accuracy: 0.4379 - val_loss: 2.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - categorical_accuracy: 0.4004 - loss: 1.9809 - val_categorical_accuracy: 0.3996 - val_loss: 1.9349\n",
      "Categorical Accuracy: 39.96%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "def small_convolutional_model():\n",
    "    model = Sequential()\n",
    "    # First Convolutional Layer\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size= (1, 17),\n",
    "            strides=(1, 1),\n",
    "            input_shape=(img_rows, img_cols, 1)  # Ensure input_shape is correctly set\n",
    "        )\n",
    "    )\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "\n",
    "    # Flatten and Dense Layers for Classification\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(33, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "# 8. Compile the Model\n",
    "small_model = small_convolutional_model()\n",
    "small_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['categorical_accuracy']\n",
    ")\n",
    "\n",
    "# (Optional) Explicitly build the model\n",
    "# model.build(input_shape=(None, img_rows, img_cols, 1))\n",
    "\n",
    "# Display Model Architecture\n",
    "small_model.summary()\n",
    "# 9. Set Up Early Stopping\n",
    "callbacks = [EarlyStopping(monitor='categorical_accuracy', patience=3, verbose=0)]\n",
    "# 10. Train the Model\n",
    "small_history = small_model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,callbacks=callbacks,validation_data=(x_test, y_test))\n",
    "\n",
    "# 11. Evaluate the Model\n",
    "small_scores = small_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Categorical Accuracy: {small_scores[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defff724-73f7-4c83-ae1b-4217c5c8e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc285fdc-59b8-4041-a7ef-bffddecac0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/50\n",
      "Batch 0: Total Loss: 5.847992420196533, Student Loss: 3.6622066497802734, Attention Loss: 2.1857857704162598\n",
      "Epoch 1, Test Loss: 0.4360, Test Accuracy: 88.57%\n",
      "Starting epoch 2/50\n",
      "Batch 0: Total Loss: 1.1259129047393799, Student Loss: 0.5283489227294922, Attention Loss: 0.5975640416145325\n",
      "Epoch 2, Test Loss: 0.2710, Test Accuracy: 91.27%\n",
      "Starting epoch 3/50\n",
      "Batch 0: Total Loss: 0.8776256442070007, Student Loss: 0.32863134145736694, Attention Loss: 0.5489943027496338\n",
      "Epoch 3, Test Loss: 0.2230, Test Accuracy: 92.82%\n",
      "Starting epoch 4/50\n",
      "Batch 0: Total Loss: 0.8457457423210144, Student Loss: 0.2890107035636902, Attention Loss: 0.5567350387573242\n",
      "Epoch 4, Test Loss: 0.1944, Test Accuracy: 93.67%\n",
      "Starting epoch 5/50\n",
      "Batch 0: Total Loss: 0.7785842418670654, Student Loss: 0.24774879217147827, Attention Loss: 0.5308354496955872\n",
      "Epoch 5, Test Loss: 0.1968, Test Accuracy: 93.59%\n",
      "Starting epoch 6/50\n",
      "Batch 0: Total Loss: 0.6658088564872742, Student Loss: 0.1741873174905777, Attention Loss: 0.49162155389785767\n",
      "Epoch 6, Test Loss: 0.1700, Test Accuracy: 94.15%\n",
      "Starting epoch 7/50\n",
      "Batch 0: Total Loss: 0.5500593185424805, Student Loss: 0.11604264378547668, Attention Loss: 0.4340166747570038\n",
      "Epoch 7, Test Loss: 0.1809, Test Accuracy: 93.52%\n",
      "Starting epoch 8/50\n",
      "Batch 0: Total Loss: 0.5918241143226624, Student Loss: 0.12850268185138702, Attention Loss: 0.4633214473724365\n",
      "Epoch 8, Test Loss: 0.1727, Test Accuracy: 94.68%\n",
      "Starting epoch 9/50\n",
      "Batch 0: Total Loss: 0.5919849872589111, Student Loss: 0.13465362787246704, Attention Loss: 0.4573313593864441\n",
      "Epoch 9, Test Loss: 0.1641, Test Accuracy: 94.60%\n",
      "Starting epoch 10/50\n",
      "Batch 0: Total Loss: 0.4657766819000244, Student Loss: 0.07283113896846771, Attention Loss: 0.3929455280303955\n",
      "Epoch 10, Test Loss: 0.1854, Test Accuracy: 94.37%\n",
      "Starting epoch 11/50\n",
      "Batch 0: Total Loss: 0.6512574553489685, Student Loss: 0.14055229723453522, Attention Loss: 0.5107051730155945\n",
      "Epoch 11, Test Loss: 0.1736, Test Accuracy: 94.94%\n",
      "Starting epoch 12/50\n",
      "Batch 0: Total Loss: 0.5197098255157471, Student Loss: 0.10137207806110382, Attention Loss: 0.41833773255348206\n",
      "Epoch 12, Test Loss: 0.2215, Test Accuracy: 93.18%\n",
      "Starting epoch 13/50\n",
      "Batch 0: Total Loss: 0.8428618311882019, Student Loss: 0.17776550352573395, Attention Loss: 0.6650963425636292\n",
      "Epoch 13, Test Loss: 0.1739, Test Accuracy: 95.20%\n",
      "Starting epoch 14/50\n",
      "Batch 0: Total Loss: 0.5046989917755127, Student Loss: 0.0830492153763771, Attention Loss: 0.4216497838497162\n",
      "Epoch 14, Test Loss: 0.2220, Test Accuracy: 93.78%\n",
      "Starting epoch 15/50\n",
      "Batch 0: Total Loss: 0.6450756788253784, Student Loss: 0.11653450131416321, Attention Loss: 0.5285411477088928\n",
      "Epoch 15, Test Loss: 0.2105, Test Accuracy: 93.37%\n",
      "Starting epoch 16/50\n",
      "Batch 0: Total Loss: 0.510508120059967, Student Loss: 0.06783895194530487, Attention Loss: 0.442669153213501\n",
      "Epoch 16, Test Loss: 0.1814, Test Accuracy: 93.77%\n",
      "Starting epoch 17/50\n",
      "Batch 0: Total Loss: 0.5064387321472168, Student Loss: 0.066153883934021, Attention Loss: 0.4402848482131958\n",
      "Epoch 17, Test Loss: 0.2010, Test Accuracy: 93.59%\n",
      "Starting epoch 18/50\n",
      "Batch 0: Total Loss: 0.4574183523654938, Student Loss: 0.04929918795824051, Attention Loss: 0.40811917185783386\n",
      "Epoch 18, Test Loss: 0.1996, Test Accuracy: 93.41%\n",
      "Starting epoch 19/50\n",
      "Batch 0: Total Loss: 0.4384326934814453, Student Loss: 0.056206390261650085, Attention Loss: 0.3822263181209564\n",
      "Epoch 19, Test Loss: 0.2611, Test Accuracy: 92.40%\n",
      "Starting epoch 20/50\n",
      "Batch 0: Total Loss: 0.47838932275772095, Student Loss: 0.039675451815128326, Attention Loss: 0.4387138783931732\n",
      "Epoch 20, Test Loss: 0.2120, Test Accuracy: 94.29%\n",
      "Starting epoch 21/50\n",
      "Batch 0: Total Loss: 0.5546881556510925, Student Loss: 0.07085506618022919, Attention Loss: 0.48383307456970215\n",
      "Epoch 21, Test Loss: 0.1801, Test Accuracy: 94.82%\n",
      "Starting epoch 22/50\n",
      "Batch 0: Total Loss: 0.3924192786216736, Student Loss: 0.05917774885892868, Attention Loss: 0.3332415223121643\n",
      "Epoch 22, Test Loss: 0.2278, Test Accuracy: 93.11%\n",
      "Starting epoch 23/50\n",
      "Batch 0: Total Loss: 0.4341370463371277, Student Loss: 0.044582463800907135, Attention Loss: 0.38955458998680115\n",
      "Epoch 23, Test Loss: 0.1648, Test Accuracy: 94.93%\n",
      "Starting epoch 24/50\n",
      "Batch 0: Total Loss: 0.3658078908920288, Student Loss: 0.036606770008802414, Attention Loss: 0.3292011320590973\n",
      "Epoch 24, Test Loss: 0.1862, Test Accuracy: 94.94%\n",
      "Starting epoch 25/50\n",
      "Batch 0: Total Loss: 0.4242111146450043, Student Loss: 0.05541563406586647, Attention Loss: 0.3687954843044281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 17:00:01.056838: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Test Loss: 0.1602, Test Accuracy: 95.38%\n",
      "Starting epoch 26/50\n",
      "Batch 0: Total Loss: 0.37355947494506836, Student Loss: 0.04627083241939545, Attention Loss: 0.3272886276245117\n",
      "Epoch 26, Test Loss: 0.1852, Test Accuracy: 95.27%\n",
      "Starting epoch 27/50\n",
      "Batch 0: Total Loss: 0.3659989833831787, Student Loss: 0.03867688402533531, Attention Loss: 0.3273220956325531\n",
      "Epoch 27, Test Loss: 0.1873, Test Accuracy: 94.89%\n",
      "Starting epoch 28/50\n",
      "Batch 0: Total Loss: 0.3727739751338959, Student Loss: 0.040163762867450714, Attention Loss: 0.33261021971702576\n",
      "Epoch 28, Test Loss: 0.1873, Test Accuracy: 95.05%\n",
      "Starting epoch 29/50\n",
      "Batch 0: Total Loss: 0.3967060446739197, Student Loss: 0.05508936569094658, Attention Loss: 0.341616690158844\n",
      "Epoch 29, Test Loss: 0.1846, Test Accuracy: 94.82%\n",
      "Starting epoch 30/50\n",
      "Batch 0: Total Loss: 0.39831361174583435, Student Loss: 0.045671701431274414, Attention Loss: 0.35264191031455994\n",
      "Epoch 30, Test Loss: 0.1956, Test Accuracy: 95.04%\n",
      "Starting epoch 31/50\n",
      "Batch 0: Total Loss: 0.4480661451816559, Student Loss: 0.06252502650022507, Attention Loss: 0.3855411112308502\n",
      "Epoch 31, Test Loss: 0.2638, Test Accuracy: 94.15%\n",
      "Starting epoch 32/50\n",
      "Batch 0: Total Loss: 0.4272310435771942, Student Loss: 0.05671627074480057, Attention Loss: 0.37051478028297424\n",
      "Epoch 32, Test Loss: 0.2346, Test Accuracy: 94.79%\n",
      "Starting epoch 33/50\n",
      "Batch 0: Total Loss: 0.47492676973342896, Student Loss: 0.06564023345708847, Attention Loss: 0.4092865288257599\n",
      "Epoch 33, Test Loss: 0.2083, Test Accuracy: 95.01%\n",
      "Starting epoch 34/50\n",
      "Batch 0: Total Loss: 0.6052672863006592, Student Loss: 0.08570578694343567, Attention Loss: 0.5195614695549011\n",
      "Epoch 34, Test Loss: 0.1840, Test Accuracy: 95.16%\n",
      "Starting epoch 35/50\n",
      "Batch 0: Total Loss: 0.46384477615356445, Student Loss: 0.06710755825042725, Attention Loss: 0.3967372179031372\n",
      "Epoch 35, Test Loss: 0.2174, Test Accuracy: 95.08%\n",
      "Starting epoch 36/50\n",
      "Batch 0: Total Loss: 0.4115868806838989, Student Loss: 0.043864522129297256, Attention Loss: 0.36772236227989197\n",
      "Epoch 36, Test Loss: 0.2830, Test Accuracy: 93.40%\n",
      "Starting epoch 37/50\n",
      "Batch 0: Total Loss: 0.5403485298156738, Student Loss: 0.05063311755657196, Attention Loss: 0.48971542716026306\n",
      "Epoch 37, Test Loss: 0.2052, Test Accuracy: 94.94%\n",
      "Starting epoch 38/50\n",
      "Batch 0: Total Loss: 0.36503076553344727, Student Loss: 0.040045589208602905, Attention Loss: 0.32498517632484436\n",
      "Epoch 38, Test Loss: 0.2103, Test Accuracy: 95.15%\n",
      "Starting epoch 39/50\n",
      "Batch 0: Total Loss: 0.3765673339366913, Student Loss: 0.04052717238664627, Attention Loss: 0.3360401690006256\n",
      "Epoch 39, Test Loss: 0.2320, Test Accuracy: 94.97%\n",
      "Starting epoch 40/50\n",
      "Batch 0: Total Loss: 0.3832298815250397, Student Loss: 0.0494566336274147, Attention Loss: 0.33377325534820557\n",
      "Epoch 40, Test Loss: 0.2348, Test Accuracy: 94.15%\n",
      "Starting epoch 41/50\n",
      "Batch 0: Total Loss: 0.4647102653980255, Student Loss: 0.049595367163419724, Attention Loss: 0.4151149094104767\n",
      "Epoch 41, Test Loss: 0.1966, Test Accuracy: 94.86%\n",
      "Starting epoch 42/50\n",
      "Batch 0: Total Loss: 0.39970263838768005, Student Loss: 0.049029577523469925, Attention Loss: 0.35067304968833923\n",
      "Epoch 42, Test Loss: 0.2267, Test Accuracy: 95.24%\n",
      "Starting epoch 43/50\n",
      "Batch 0: Total Loss: 0.36169469356536865, Student Loss: 0.03869102895259857, Attention Loss: 0.3230036795139313\n",
      "Epoch 43, Test Loss: 0.2059, Test Accuracy: 94.75%\n",
      "Starting epoch 44/50\n",
      "Batch 0: Total Loss: 0.369756281375885, Student Loss: 0.04582134634256363, Attention Loss: 0.323934942483902\n",
      "Epoch 44, Test Loss: 0.2236, Test Accuracy: 94.19%\n",
      "Starting epoch 45/50\n",
      "Batch 0: Total Loss: 0.4297848045825958, Student Loss: 0.044736944139003754, Attention Loss: 0.3850478529930115\n",
      "Epoch 45, Test Loss: 0.2562, Test Accuracy: 93.93%\n",
      "Starting epoch 46/50\n",
      "Batch 0: Total Loss: 0.3930201828479767, Student Loss: 0.04237845540046692, Attention Loss: 0.35064172744750977\n",
      "Epoch 46, Test Loss: 0.2310, Test Accuracy: 94.71%\n",
      "Starting epoch 47/50\n",
      "Batch 0: Total Loss: 0.3943328857421875, Student Loss: 0.04013094678521156, Attention Loss: 0.35420194268226624\n",
      "Epoch 47, Test Loss: 0.2204, Test Accuracy: 95.31%\n",
      "Starting epoch 48/50\n",
      "Batch 0: Total Loss: 0.3660426139831543, Student Loss: 0.04207838326692581, Attention Loss: 0.3239642381668091\n",
      "Epoch 48, Test Loss: 0.1899, Test Accuracy: 94.93%\n",
      "Starting epoch 49/50\n",
      "Batch 0: Total Loss: 0.3877128064632416, Student Loss: 0.04435984417796135, Attention Loss: 0.3433529734611511\n",
      "Epoch 49, Test Loss: 0.2072, Test Accuracy: 95.05%\n",
      "Starting epoch 50/50\n",
      "Batch 0: Total Loss: 0.369994580745697, Student Loss: 0.04284664988517761, Attention Loss: 0.3271479308605194\n",
      "Epoch 50, Test Loss: 0.2319, Test Accuracy: 95.08%\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "@tf.function\n",
    "\n",
    "def attention_transfer_loss_cosine_similarity(teacher_saliency_map, student_saliency_map):\n",
    "    # Ensure tensors are float32\n",
    "    teacher_saliency_map = tf.cast(teacher_saliency_map, tf.float32)\n",
    "    student_saliency_map = tf.cast(student_saliency_map, tf.float32)\n",
    "    # Compute dot product and magnitudes\n",
    "    dot_product = tf.reduce_sum(teacher_saliency_map * student_saliency_map, axis=[1,2,3])\n",
    "    magnitude_teacher = tf.sqrt(tf.reduce_sum(tf.square(teacher_saliency_map), axis=[1,2,3]))\n",
    "    magnitude_student = tf.sqrt(tf.reduce_sum(tf.square(student_saliency_map), axis=[1,2,3]))\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (magnitude_teacher * magnitude_student + 1e-8)  # Adding epsilon for numerical stability\n",
    "    return -tf.reduce_mean(cosine_similarity)\n",
    "\n",
    "@tf.function\n",
    "\n",
    "def attention_transfer_loss_top10_euclidean_distance_both(teacher_saliency_map, student_saliency_map):\n",
    "    # Ensure tensors are float32\n",
    "    teacher_saliency_map = tf.cast(teacher_saliency_map, tf.float32)\n",
    "    student_saliency_map = tf.cast(student_saliency_map, tf.float32)\n",
    "    # Flatten the spatial dimensions\n",
    "    batch_size = tf.shape(teacher_saliency_map)[0]\n",
    "    teacher_flat = tf.reshape(teacher_saliency_map, [batch_size, -1])\n",
    "    student_flat = tf.reshape(student_saliency_map, [batch_size, -1])\n",
    "    num_pixels = tf.shape(teacher_flat)[1]\n",
    "    num_top_values = tf.cast(0.99 * tf.cast(num_pixels, tf.float32), tf.int32)\n",
    "    num_bottom_values = tf.cast(0.01 * tf.cast(num_pixels, tf.float32), tf.int32)\n",
    "    # Use top_k for top values\n",
    "    top_values, _ = tf.math.top_k(teacher_flat, k=num_top_values, sorted=True)\n",
    "    # For bottom values, get the smallest k values by negating\n",
    "    bottom_values, _ = tf.math.top_k(-teacher_flat, k=num_bottom_values, sorted=True)\n",
    "    bottom_values = -bottom_values  # Revert to original values\n",
    "    # Gather corresponding student values\n",
    "    # To find indices, use argsort\n",
    "    sorted_indices = tf.argsort(teacher_flat, axis=1, direction='ASCENDING')\n",
    "    top_indices = sorted_indices[:, -num_top_values:]\n",
    "    bottom_indices = sorted_indices[:, :num_bottom_values]\n",
    "    # Use batch gather\n",
    "    batch_indices = tf.reshape(tf.range(batch_size), [-1, 1])\n",
    "    batch_indices = tf.tile(batch_indices, [1, num_top_values])\n",
    "    top_gather_indices = tf.stack([batch_indices, top_indices], axis=-1)\n",
    "    student_top_values = tf.gather_nd(student_flat, top_gather_indices)\n",
    "    batch_indices_bottom = tf.reshape(tf.range(batch_size), [-1, 1])\n",
    "    batch_indices_bottom = tf.tile(batch_indices_bottom, [1, num_bottom_values])\n",
    "    bottom_gather_indices = tf.stack([batch_indices_bottom, bottom_indices], axis=-1)\n",
    "    student_bottom_values = tf.gather_nd(student_flat, bottom_gather_indices)\n",
    "    # Calculate Euclidean distance for top and bottom values\n",
    "    euclidean_top = tf.sqrt(tf.reduce_sum(tf.square(top_values - student_top_values), axis=1))\n",
    "    euclidean_bottom = tf.sqrt(tf.reduce_sum(tf.square(bottom_values - student_bottom_values), axis=1))\n",
    "    # Total loss\n",
    "    total_euclidean_distance = tf.reduce_mean(euclidean_top + euclidean_bottom)\n",
    "    return total_euclidean_distance\n",
    "\n",
    "# Precompute teacher saliency maps using vectorized operations\n",
    "\n",
    "teacher_saliency_map = []\n",
    "for batch_inputs, batch_labels in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(batch_inputs)\n",
    "        teacher_outputs = model(batch_inputs, training=True)\n",
    "        teacher_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(batch_labels, teacher_outputs))\n",
    "    teacher_grads = tape.gradient(teacher_loss, batch_inputs)\n",
    "    teacher_saliency_map.append(teacher_grads)\n",
    "\n",
    "# Define and compile the student model\n",
    "conv_model2 = convolutional_model()\n",
    "conv_model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy']\n",
    ")\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Training loop with @tf.function for graph execution\n",
    "\n",
    "@tf.function\n",
    "\n",
    "def train_step(conv_model, batch_inputs, batch_labels, teacher_saliency):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass for student\n",
    "        student_outputs = conv_model(batch_inputs, training=True)\n",
    "        student_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.categorical_crossentropy(batch_labels, student_outputs)\n",
    "        )\n",
    "\n",
    "        # Compute student saliency maps\n",
    "        with tf.GradientTape() as tape_student:\n",
    "            tape_student.watch(batch_inputs)\n",
    "            student_outputs = conv_model(batch_inputs, training=True)\n",
    "            student_loss_inner = tf.reduce_mean(\n",
    "                tf.keras.losses.categorical_crossentropy(batch_labels, student_outputs)\n",
    "            )\n",
    "        student_grads = tape_student.gradient(student_loss_inner, batch_inputs)\n",
    "\n",
    "        # Compute attention loss\n",
    "        att_loss = attention_transfer_loss_top10_euclidean_distance_both(teacher_saliency, student_grads)*1000\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = student_loss + att_loss\n",
    "\n",
    "    # Compute gradients and apply\n",
    "    gradients = tape.gradient(total_loss, conv_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, conv_model.trainable_variables))\n",
    "    return total_loss, student_loss, att_loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Starting epoch {epoch+1}/{EPOCHS}\")\n",
    "    batch_index = 0\n",
    "    for batch_inputs, batch_labels in train_dataset:\n",
    "        # Fetch precomputed teacher saliency map\n",
    "        teacher_saliency = teacher_saliency_map[batch_index]\n",
    "        # Perform a training step\n",
    "        total_loss, student_loss, att_loss = train_step(conv_model2, batch_inputs, batch_labels, teacher_saliency)\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f\"Batch {batch_index}: Total Loss: {total_loss.numpy()}, \"\n",
    "                  f\"Student Loss: {student_loss.numpy()}, Attention Loss: {att_loss.numpy()}\")\n",
    "        batch_index += 1\n",
    "    # Evaluation on test dataset\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    for test_inputs, test_labels in test_dataset:\n",
    "        test_outputs = conv_model2(test_inputs, training=False)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(test_labels, test_outputs))\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.keras.metrics.categorical_accuracy(test_labels, test_outputs)\n",
    "        )\n",
    "        test_loss += loss\n",
    "        test_accuracy += accuracy\n",
    "        num_batches += 1\n",
    "    test_loss /= num_batches\n",
    "    test_accuracy /= num_batches\n",
    "    print(f'Epoch {epoch + 1}, Test Loss: {test_loss.numpy():.4f}, Test Accuracy: {test_accuracy.numpy():.2%}')\n",
    "    '''if test_accuracy.numpy() > 0.85:\n",
    "        print(\"Reached target accuracy. Stopping training.\")\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa498a-391c-49f0-8dff-398e33aa344e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
